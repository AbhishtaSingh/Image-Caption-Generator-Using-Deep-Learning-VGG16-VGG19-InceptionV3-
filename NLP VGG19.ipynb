{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.models import Model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_path = \"Flickr8k.token.txt\"\n",
    "train_images_path = \"Flickr_8k.trainImages.txt\"\n",
    "test_images_path = \"Flickr_8k.testImages.txt\"\n",
    "images_path = r\"C:\\Users\\Abhishta\\Desktop\\Image Caption Generator\\Flicker8K_Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the caption file & read it\n",
    "def load_caption_file(path):\n",
    "    \n",
    "    # dictionary to store captions\n",
    "    captions_dict = {}\n",
    "    \n",
    "    # iterate through the file\n",
    "    for caption in open(path):\n",
    "    \n",
    "        # caption has format-> 109202801_c6381eef15.jpg#0  Two draft horses pull a cart through the snow .\n",
    "        tokens = caption.split()\n",
    "        caption_id, caption_text = tokens[0].split('.')[0], tokens[1:]\n",
    "        caption_text = ' '.join(caption_text)\n",
    "        \n",
    "        # save it in the captions dictionary\n",
    "        if caption_id not in captions_dict:\n",
    "            captions_dict[caption_id] = caption_text\n",
    "        \n",
    "    return captions_dict\n",
    "\n",
    "# call the function\n",
    "captions_dict = load_caption_file(\"captions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the captions\n",
    "import string\n",
    "\n",
    "# dictionary to store the cleaned captions\n",
    "new_captions_dict = {}\n",
    "\n",
    "# prepare translation table for removing punctuation. third argument is the list of punctuations we want to remove\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# loop through the dictionary\n",
    "for caption_id, caption_text in captions_dict.items():\n",
    "    # tokenize the caption_text\n",
    "    caption_text = caption_text.split()\n",
    "    # convert it into lower case\n",
    "    caption_text = [token.lower() for token in caption_text]\n",
    "    # remove punctuation from each token\n",
    "    caption_text = [token.translate(table) for token in caption_text]\n",
    "    # remove all the single letter tokens like 'a', 's'\n",
    "    caption_text = [token for token in caption_text if len(token)>1]\n",
    "    # store the cleaned captions\n",
    "    new_captions_dict[caption_id] = 'startseq ' + ' '.join(caption_text) + ' endseq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete unwanted \n",
    "del captions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"image,caption\" : startseq  endseq\n"
     ]
    }
   ],
   "source": [
    "print('\"' + list(new_captions_dict.keys())[0] + '\"' + ' : ' + new_captions_dict[list(new_captions_dict.keys())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8092"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_captions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1000268201_693b08cb0e'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_images_list = []\n",
    "\n",
    "image_index = list(new_captions_dict.keys())\n",
    "\n",
    "caption_images_list = [ image.split('.')[0] for image in os.listdir(images_path) if image.split('.')[0] in image_index ]\n",
    "caption_images_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['98377566_e4674d1ebd',\n",
       " '985067019_705fe4a4cc',\n",
       " '987907964_5a06a63609',\n",
       " '989754491_7e53fb4586',\n",
       " '989851184_9ef368e520',\n",
       " '990890291_afc72be141',\n",
       " '99171998_7cc800ceef',\n",
       " '99679241_adc853a5c0',\n",
       " '997338199_7343367d7f',\n",
       " '997722733_0cb5439472']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_validate_images = caption_images_list[0:8081]  \n",
    "test_images = caption_images_list[8081:8091]\n",
    "test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from each photo in the directory\n",
    "def extract_features(directory, image_keys):\n",
    "    # load the model\n",
    "    model = VGG19()\n",
    "    \n",
    "    # re-structure the model\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    \n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "    \n",
    "    # extract features from each photo\n",
    "    features = dict()\n",
    "    \n",
    "    for name in image_keys:\n",
    "        \n",
    "        # load an image from file\n",
    "        filename = directory + '/' + name + '.jpg'\n",
    "        \n",
    "        # load the image and convert it into target size of 224*224\n",
    "        image = load_img(filename, target_size=(224, 224))\n",
    "        \n",
    "        # convert the image pixels to a numpy array\n",
    "        image = img_to_array(image)\n",
    "        \n",
    "        # reshape data for the model\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        \n",
    "        # prepare the image for the VGG model\n",
    "        image = preprocess_input(image)\n",
    "        \n",
    "        # get features\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        \n",
    "        # get image id\n",
    "        image_id = name.split('.')[0]\n",
    "        \n",
    "        # store feature\n",
    "        features[image_id] = feature\n",
    "        \n",
    "#         print('>%s' % name)\n",
    "        \n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 139,570,240\n",
      "Trainable params: 139,570,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# extracting image features for train_validate_images\n",
    "train_validate_features = extract_features(images_path, train_validate_images) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000268201_693b08cb0e : [[0.        1.8022311 0.        ... 0.        0.        0.       ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"{} : {}\".format(list(train_validate_features.keys())[0], train_validate_features[list(train_validate_features.keys())[0]] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8081"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_validate_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "dump(train_validate_features, open('./train_validate_features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import numpy as np\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Dropout, LSTM, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8081"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a dictionary of image with caption for train_validate_images\n",
    "train_validate_image_caption = {}\n",
    "\n",
    "for image, caption in new_captions_dict.items():\n",
    "    \n",
    "    # check whether the image is available in both train_validate_images list and train_validate_features dictionary\n",
    "    if image in train_validate_images and image in list(train_validate_features.keys()):\n",
    "        \n",
    "         train_validate_image_caption.update({image : caption})\n",
    "\n",
    "len(train_validate_image_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'startseq black dog and spotted dog are fighting endseq'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_validate_image_caption.values())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# create word count dictionary on the captions list\n",
    "tokenizer.fit_on_texts(list(train_validate_image_caption.values()))\n",
    "\n",
    "# how many words are there in the vocabulary? store the total length in vocab_len and add 1 because word_index starts with 1 not 0 \n",
    "vocab_len = len(tokenizer.word_index) + 1\n",
    "\n",
    "# store the length of the maximum sentence\n",
    "max_len = max(len(train_validate_image_caption[image].split()) for image in train_validate_image_caption)\n",
    "\n",
    "def prepare_data(image_keys):\n",
    "    \n",
    "    # x1 will store the image feature, x2 will store one sequence and y will store the next sequence\n",
    "    x1, x2, y = [], [], []\n",
    "\n",
    "    # iterate through all the images \n",
    "    for image in image_keys:\n",
    "\n",
    "        # store the caption of that image\n",
    "        caption = train_validate_image_caption[image]\n",
    "\n",
    "        # split the image into tokens\n",
    "        caption = caption.split()\n",
    "\n",
    "        # generate integer sequences of the\n",
    "        seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "\n",
    "        length = len(seq)\n",
    "\n",
    "        for i in range(1, length):\n",
    "\n",
    "            x2_seq, y_seq = seq[:i] , seq[i]  \n",
    "\n",
    "            # pad the sequences\n",
    "            x2_seq = pad_sequences([x2_seq], maxlen = max_len)[0]\n",
    "\n",
    "\n",
    "            # encode the output sequence                \n",
    "            y_seq = to_categorical([y_seq], num_classes = vocab_len)[0]\n",
    "\n",
    "            x1.append( train_validate_features[image][0] )\n",
    "\n",
    "            x2.append(x2_seq)\n",
    "\n",
    "            y.append(y_seq)\n",
    "               \n",
    "    return np.array(x1), np.array(x2), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x1, train_x2, train_y = prepare_data( train_validate_images[0:7081] )\n",
    "validate_x1, validate_x2, validate_y = prepare_data( train_validate_images[7081:8081] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71355"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10160"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validate_x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 30)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 30, 256)      1143296     ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096)         0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 30, 256)      0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          1048832     ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 256)          525312      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 256)          0           ['dense[0][0]',                  \n",
      "                                                                  'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 256)          65792       ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4466)         1147762     ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,930,994\n",
      "Trainable params: 3,930,994\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# feature extractor model\n",
    "input_1 = Input(shape=(4096,))\n",
    "droplayer = Dropout(0.5)(input_1)\n",
    "denselayer = Dense(256, activation='relu')(droplayer)\n",
    "\n",
    "# sequence model\n",
    "input_2 = Input(shape=(max_len,))\n",
    "embedding = Embedding(vocab_len, 256, mask_zero=True)(input_2)\n",
    "droplayer_ = Dropout(0.5)(embedding)\n",
    "lstm = LSTM(256)(droplayer_)\n",
    "\n",
    "# decoder model\n",
    "decoder1 = add([denselayer, lstm])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_len, activation='softmax')(decoder2)\n",
    "\n",
    "# tie it together [image, seq] [word]\n",
    "model = Model(inputs=[input_1, input_2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# summarize model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in c:\\programdata\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydot) (2.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\programdata\\anaconda3\\lib\\site-packages (0.16)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define checkpoint callback\n",
    "filepath = './model-ep{epoch:02d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "\n",
    "callbacks = [\n",
    "             ModelCheckpoint(filepath= filepath, save_best_only=True, monitor='val_loss') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_x1  (71355, 4096)\n",
      "shape of train_x2  (71355, 30)\n",
      "shape of train_y  (71355, 4466)\n",
      "\n",
      "shape of validate_x1  (10160, 4096)\n",
      "shape of validate_x2  (10160, 30)\n",
      "shape of validate_y  (10160, 4466)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of train_x1 \", train_x1.shape)\n",
    "print(\"shape of train_x2 \", train_x2.shape)\n",
    "print(\"shape of train_y \", train_y.shape)\n",
    "print()\n",
    "print(\"shape of validate_x1 \", validate_x1.shape)\n",
    "print(\"shape of validate_x2 \", validate_x2.shape)\n",
    "print(\"shape of validate_y \", validate_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2230/2230 [==============================] - 251s 111ms/step - loss: 5.0537 - val_loss: 4.4831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishta\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "2230/2230 [==============================] - 253s 113ms/step - loss: 4.2247 - val_loss: 4.2939\n",
      "Epoch 3/20\n",
      "2230/2230 [==============================] - 246s 111ms/step - loss: 3.8860 - val_loss: 4.3077\n",
      "Epoch 4/20\n",
      "2230/2230 [==============================] - 240s 108ms/step - loss: 3.6563 - val_loss: 4.2871\n",
      "Epoch 5/20\n",
      "2230/2230 [==============================] - 239s 107ms/step - loss: 3.4822 - val_loss: 4.3396\n",
      "Epoch 6/20\n",
      "2230/2230 [==============================] - 239s 107ms/step - loss: 3.3385 - val_loss: 4.4886\n",
      "Epoch 7/20\n",
      "2230/2230 [==============================] - 240s 108ms/step - loss: 3.2149 - val_loss: 4.6050\n",
      "Epoch 8/20\n",
      "2230/2230 [==============================] - 240s 108ms/step - loss: 3.1086 - val_loss: 4.6444\n",
      "Epoch 9/20\n",
      "2230/2230 [==============================] - 249s 111ms/step - loss: 3.0110 - val_loss: 4.7894\n",
      "Epoch 10/20\n",
      "2230/2230 [==============================] - 246s 110ms/step - loss: 2.9240 - val_loss: 5.0182\n",
      "Epoch 11/20\n",
      "2230/2230 [==============================] - 231s 104ms/step - loss: 2.8442 - val_loss: 5.0287\n",
      "Epoch 12/20\n",
      "2230/2230 [==============================] - 222s 100ms/step - loss: 2.7751 - val_loss: 5.2453\n",
      "Epoch 13/20\n",
      "2230/2230 [==============================] - 246s 110ms/step - loss: 2.7116 - val_loss: 5.3107\n",
      "Epoch 14/20\n",
      "2230/2230 [==============================] - 253s 114ms/step - loss: 2.6526 - val_loss: 5.4805\n",
      "Epoch 15/20\n",
      "2230/2230 [==============================] - 253s 113ms/step - loss: 2.5996 - val_loss: 5.4923\n",
      "Epoch 16/20\n",
      "2230/2230 [==============================] - 243s 109ms/step - loss: 2.5429 - val_loss: 5.6929\n",
      "Epoch 17/20\n",
      "2230/2230 [==============================] - 248s 111ms/step - loss: 2.4969 - val_loss: 5.7388\n",
      "Epoch 18/20\n",
      "2230/2230 [==============================] - 246s 110ms/step - loss: 2.4534 - val_loss: 5.9081\n",
      "Epoch 19/20\n",
      "2230/2230 [==============================] - 229s 103ms/step - loss: 2.4145 - val_loss: 5.9600\n",
      "Epoch 20/20\n",
      "2230/2230 [==============================] - 223s 100ms/step - loss: 2.3767 - val_loss: 6.0385\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "history = model.fit([train_x1, train_x2],  \n",
    "                    train_y,              \n",
    "                    verbose = 1,            \n",
    "                    epochs = 20,            \n",
    "                    callbacks = callbacks, \n",
    "                    validation_data=([validate_x1, validate_x2], validate_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training loss and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['training', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishta\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    }
   ],
   "source": [
    "# saving the model with last parameter \n",
    "model.save('./latest_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from each photo in the directory\n",
    "def extract_feat(filename):\n",
    "    # load the model\n",
    "    model = VGG19()\n",
    "    # re-structure the model\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    # load the photo\n",
    "    image = load_img(filename, target_size=(224, 224))\n",
    "    # convert the image pixels to a numpy array\n",
    "    image = img_to_array(image)\n",
    "    # reshape data for the model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # prepare the image for the VGG model\n",
    "    image = preprocess_input(image)\n",
    "    # get features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    return feature\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizr):\n",
    "    for word, index in tokenizr.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = np.argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x179a2836348>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.keras.applications.VGG19(\n",
    "    include_top=True,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=None,\n",
    "    pooling=None,\n",
    "    classes=1000,\n",
    "    classifier_activation=\"softmax\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted caption ->  startseq man in black shirt is sitting on bench in front of christmas tree endseq\n",
      "\n",
      "Actual caption ->  startseq child in pink dress is climbing up set of stairs in an entry way endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq black and white dog is jumping in the air endseq\n",
      "\n",
      "Actual caption ->  startseq black dog and spotted dog are fighting endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq boy in blue shirt is jumping off the street endseq\n",
      "\n",
      "Actual caption ->  startseq little girl covered in paint sits in front of painted rainbow with her hands in bowl endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq black and white dog is jumping in the air endseq\n",
      "\n",
      "Actual caption ->  startseq man lays on bench while his dog sits by him endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq man in black jacket is standing in front of reception lit train endseq\n",
      "\n",
      "Actual caption ->  startseq man in an orange hat starring at something endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq boy in green shirt is sitting on the grass endseq\n",
      "\n",
      "Actual caption ->  startseq child playing on rope net endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq black and white dog is running in field endseq\n",
      "\n",
      "Actual caption ->  startseq black and white dog is running in grassy garden surrounded by white fence endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq brown dog is running through the water endseq\n",
      "\n",
      "Actual caption ->  startseq dog shakes its head near the shore red ball next to it endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq boy in red shirt is jumping on the street endseq\n",
      "\n",
      "Actual caption ->  startseq boy smiles in front of stony wall in city endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq black and white dog is running through the grass endseq\n",
      "\n",
      "Actual caption ->  startseq black dog leaps over log endseq\n",
      "*********************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "# load the model\n",
    "modl = load_model('./latest_model.h5')\n",
    "\n",
    "# generate description\n",
    "tokenizr = Tokenizer()\n",
    "tokenizr.fit_on_texts([caption for image, caption in new_captions_dict.items() if image in train_validate_images])\n",
    "max_length = 30\n",
    "\n",
    "for count in range(10):\n",
    "\n",
    "    photo = extract_feat('{}.jpg'.format(images_path+'/'+train_validate_images[count]))  \n",
    "\n",
    "    # generate description\n",
    "    description = generate_desc(modl, tokenizr, photo, max_length)\n",
    "    print(\"Predicted caption -> \", description)\n",
    "    print()\n",
    "    print(\"Actual caption -> \", new_captions_dict[train_validate_images[count]])\n",
    "    print('*********************************************************************')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted caption ->  startseq lift brown on goggles and in bicycle endseq\n",
      "\n",
      "Actual caption ->  startseq brown and white dogs lift their ears and look through fence endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq man and boy grass on surrounded white in in in in in in in in in in in in in in in in in in in in in in in\n",
      "\n",
      "Actual caption ->  startseq boy goes down an inflatable slide endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq his brown on goggles and in bicycle endseq\n",
      "\n",
      "Actual caption ->  startseq brown and white dog chasing black and white dog through the grass endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq dogs and in fence and in fence endseq\n",
      "\n",
      "Actual caption ->  startseq girl in pool wearing goggles and surrounded by other children endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq his brown on goggles and in fence endseq\n",
      "\n",
      "Actual caption ->  startseq black dog has dumbbell in his mouth endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq black on dumbbell white in walls endseq\n",
      "\n",
      "Actual caption ->  startseq man does wheelie on his bicycle on the sidewalk endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq black and boy grass on on on on on on on on on on on on on on on on on on on on on on on on on on\n",
      "\n",
      "Actual caption ->  startseq group is sitting around snowy crevasse endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq his brown on goggles and in fence endseq\n",
      "\n",
      "Actual caption ->  startseq grey bird stands majestically on beach while waves roll in endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq black and stands grass on surrounded and in in walls endseq\n",
      "\n",
      "Actual caption ->  startseq person stands near golden walls endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq black and boy grass on dumbbell white in in in in in in in in in in in in in in in in in in in in in in in\n",
      "\n",
      "Actual caption ->  startseq man in pink shirt climbs rock face endseq\n",
      "*********************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "modl = load_model('model-ep01-loss5.054-val_loss4.483.h5')\n",
    "\n",
    "# generate description\n",
    "tokenizr = Tokenizer()\n",
    "tokenizr.fit_on_texts([caption for image, caption in new_captions_dict.items() if image in test_images])\n",
    "max_length = 30\n",
    "\n",
    "for count in range(10):\n",
    "\n",
    "    photo = extract_feat('{}.jpg'.format(images_path+'/'+test_images[count]))  \n",
    "\n",
    "    # generate description\n",
    "    description = generate_desc(modl, tokenizr, photo, max_length)\n",
    "    print(\"Predicted caption -> \", description)\n",
    "    print()\n",
    "    print(\"Actual caption -> \", new_captions_dict[test_images[count]])\n",
    "    print('*********************************************************************')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted caption ->  startseq lift brown on goggles by in bicycle endseq\n",
      "\n",
      "Actual caption ->  startseq brown and white dogs lift their ears and look through fence endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq man and boy grass on surrounded white in\n",
      "\n",
      "Actual caption ->  startseq boy goes down an inflatable slide endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq lift brown on goggles by in sidewalk endseq\n",
      "\n",
      "Actual caption ->  startseq brown and white dog chasing black and white dog through the grass endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq dogs and face grass on wearing and in fence endseq\n",
      "\n",
      "Actual caption ->  startseq girl in pool wearing goggles and surrounded by other children endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq his brown on goggles by in sidewalk endseq\n",
      "\n",
      "Actual caption ->  startseq black dog has dumbbell in his mouth endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq black and boy grass on surrounded white in walls endseq\n",
      "\n",
      "Actual caption ->  startseq man does wheelie on his bicycle on the sidewalk endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq black and boy grass on surrounded white in walls endseq\n",
      "\n",
      "Actual caption ->  startseq group is sitting around snowy crevasse endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq his brown on goggles by in fence endseq\n",
      "\n",
      "Actual caption ->  startseq grey bird stands majestically on beach while waves roll in endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq black and his grass on surrounded and in\n",
      "\n",
      "Actual caption ->  startseq person stands near golden walls endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq black and boy grass on surrounded white in fence endseq\n",
      "\n",
      "Actual caption ->  startseq man in pink shirt climbs rock face endseq\n",
      "*********************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "modl = load_model('model-ep02-loss4.225-val_loss4.294.h5')\n",
    "\n",
    "# generate description\n",
    "tokenizr = Tokenizer()\n",
    "tokenizr.fit_on_texts([caption for image, caption in new_captions_dict.items() if image in test_images])\n",
    "max_length = 30\n",
    "\n",
    "for count in range(10):\n",
    "\n",
    "    photo = extract_feat('{}.jpg'.format(images_path+'/'+test_images[count]))  \n",
    "\n",
    "    # generate description\n",
    "    description = generate_desc(modl, tokenizr, photo, max_length)\n",
    "    print(\"Predicted caption -> \", description)\n",
    "    print()\n",
    "    print(\"Actual caption -> \", new_captions_dict[test_images[count]])\n",
    "    print('*********************************************************************')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted caption ->  startseq lift brown on goggles by in bicycle endseq\n",
      "\n",
      "Actual caption ->  startseq brown and white dogs lift their ears and look through fence endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq man and slide grass on around white in roll endseq\n",
      "\n",
      "Actual caption ->  startseq boy goes down an inflatable slide endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq his dog stands brown on goggles by in bicycle endseq\n",
      "\n",
      "Actual caption ->  startseq brown and white dog chasing black and white dog through the grass endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq man and slide\n",
      "\n",
      "Actual caption ->  startseq girl in pool wearing goggles and surrounded by other children endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq brown on goggles by in roll endseq\n",
      "\n",
      "Actual caption ->  startseq black dog has dumbbell in his mouth endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq black and boy grass on\n",
      "\n",
      "Actual caption ->  startseq man does wheelie on his bicycle on the sidewalk endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq black and boy\n",
      "\n",
      "Actual caption ->  startseq group is sitting around snowy crevasse endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq his brown on goggles by in fence endseq\n",
      "\n",
      "Actual caption ->  startseq grey bird stands majestically on beach while waves roll in endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq black and his grass on surrounded white in\n",
      "\n",
      "Actual caption ->  startseq person stands near golden walls endseq\n",
      "*********************************************************************\n",
      "\n",
      "Predicted caption ->  startseq black and boy grass on\n",
      "\n",
      "Actual caption ->  startseq man in pink shirt climbs rock face endseq\n",
      "*********************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "modl = load_model('model-ep04-loss3.656-val_loss4.287.h5')\n",
    "\n",
    "# generate description\n",
    "tokenizr = Tokenizer()\n",
    "tokenizr.fit_on_texts([caption for image, caption in new_captions_dict.items() if image in test_images])\n",
    "max_length = 30\n",
    "\n",
    "for count in range(10):\n",
    "\n",
    "    photo = extract_feat('{}.jpg'.format(images_path+'/'+test_images[count]))  \n",
    "\n",
    "    # generate description\n",
    "    description = generate_desc(modl, tokenizr, photo, max_length)\n",
    "    print(\"Predicted caption -> \", description)\n",
    "    print()\n",
    "    print(\"Actual caption -> \", new_captions_dict[test_images[count]])\n",
    "    print('*********************************************************************')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
